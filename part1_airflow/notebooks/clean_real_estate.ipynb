{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Очистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведен анализ входных данных на наличие дубликатов, пропусков и выбросов. Разработанные функции в дальнейшем используются для создания DAGа, который очищает датасет, созданный на прошлом этапе. Код сопровождается комментариями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключение к БД и загрузка датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "dst_host = os.environ.get('DB_DESTINATION_HOST')\n",
    "dst_port = os.environ.get('DB_DESTINATION_PORT')\n",
    "dst_username = os.environ.get('DB_DESTINATION_USER')\n",
    "dst_password = os.environ.get('DB_DESTINATION_PASSWORD')\n",
    "dst_db = os.environ.get('DB_DESTINATION_NAME')\n",
    "                        \n",
    "dst_conn = create_engine(f'postgresql://{dst_username}:{dst_password}@{dst_host}:{dst_port}/{dst_db}')\n",
    "\n",
    "sql = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM real_estate\n",
    "\"\"\"\n",
    "data = pd.read_sql(sql, dst_conn)\n",
    "\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дубликаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий код проверяет, являются ли все 'flat_id' уникальны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_duplicated_id = data.duplicated(subset=['flat_id'], keep=False)\n",
    "print(sum(is_duplicated_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество дубликатов равно 0, следовательно, все flat_id уникальны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, ищем строки, где все переменные равны, но flat_id при этом отличается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data):\n",
    "    feature_cols = data.columns.drop('flat_id').tolist()\n",
    "    is_duplicated_features = data.duplicated(subset=feature_cols, keep=False)\n",
    "    data = data[~is_duplicated_features].reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "remove_duplicates(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таковых не обнаружено. Количество строк все еще 141362."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пропуски"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество пропусков в каждой колонке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ни в одной из колонок пропусков не обнаружено."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, очистим датасет от наблюдений, которые значительно выбиваются из общего паттерна. Для этого выделим все колонки, которые имеют тип данных 'float':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = data.select_dtypes(include=['float'])\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения задачи воспользуемся методом IRQ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.5 # Нормирующий коэффициент\n",
    "potential_outliers = pd.DataFrame()\n",
    "\n",
    "for col in num_features:\n",
    "\tQ1 = data[col].quantile(0.25)\n",
    "\tQ3 = data[col].quantile(0.75)\n",
    "\tIQR = Q3 - Q1\n",
    "\tmargin = threshold * IQR\n",
    "\tlower = Q1 - margin # Нижняя граница\n",
    "\tupper = Q3 + margin # Верхняя граница\n",
    "\tpotential_outliers[col] = ~data[col].between(lower, upper)\n",
    "\n",
    "outliers = potential_outliers.any(axis=1)\n",
    "\n",
    "print(data[outliers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обнаружено 20674 строки, которые выбиваются из общего ряда. Удалим их из датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_remove = data[outliers]['id'].tolist()\n",
    "print(rows_to_remove)\n",
    "filtered_df = data[~data['id'].isin(rows_to_remove)]\n",
    "print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговый датасет имеет 120688 строк. Данные готовы для использования."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
